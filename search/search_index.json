{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"course_schedule/","text":"start end topic 10:00 10:30 coffee! 12:00 13:00 lunch! Generate markdown tables at tablesgenerator.com","title":"Course schedule"},{"location":"Annotation/week5/","text":"","title":"Week 5"},{"location":"Assembly/week1/","text":"Using software On the IBU cluster software is available in modules or containers. More info on how to use modules here . If you need software, try to find it at the vital-it website . You can load the software modules with module add [PATH] , to make it available: module add UHTS/Quality_control/fastqc/0.11.9 fastqc --help Raw input reads work to be done here We will probably make the reads available in a central place (and have multiple accessions). PacBio reads of Ler are at ERR3415825 and ERR3415826 module add UHTS/Analysis/sratoolkit/2.10.7 for SRA_ID in ERR3415825 ERR3415826 do prefetch $SRA_ID fastq-dump \\ --gzip \\ $SRA_ID done The whole genome Illumina reads of Ler: prefetch ERR3624574 fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files ERR3624574 Download also Illumina the paired-end RNA-seq reads of Ler: for SRA_ID in SRR5230991 SRR5230992 SRR5230993 do prefetch $SRA_ID fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files $SRA_ID done Basic read statistics You have the following reads available: Whole genome PacBio reads Whole genome Illumina reads Whole transcriptome Illumina reads Exercise: Run fastqc on each of the fastq files. To do this, first look up the required module at https://www.vital-it.ch/services , and create a script to run fastqc using SLURM\u2019s sbatch . using SLURM A tutorial about job submission using SLURM on the IBU cluster you can find here . Questions: What are the read lengths of the different datasets? What kind of coverage do you expect from the Pacbio and the Illumina WGS reads? ( hint : lookup the expected genome size of Arabidopsis thaliana ) Do all datasets have information on base quality? Perform k-mer counting Exercise: Go to http://qb.cshl.edu/genomescope/ , and follow their suggestions on counting k-mers ( jellyfish count ) and creating a histogram ( jellyfish histo ). Other than what is suggested on the genomescope website, you can use the options -s 5G (hash of 5Gb) and -t 4 (4 threads). The other options you can keep the same as suggested on the website. Again, submit the commands from within a script using sbatch . jellyfish count only accepts uncompressed input Your fastq files are compressed (i.e. ending with fastq.gz ). jellyfish can not read those directly, but you can provide the files with process substitution of zcat , i.e.: jellyfish count \\ [ OPTIONS ] \\ < ( zcat myreads.fastq.gz ) \\ < ( zcat myotherreads.fastq.gz ) Questions: Upload your histo file to the website, checkout the plots and answer the following questions: Why are we using canonical k-mers? Is the estimated genome size expected? Is the percentage of heterozygousity expected?","title":"Week 1 - reads & QC"},{"location":"Assembly/week1/#using-software","text":"On the IBU cluster software is available in modules or containers. More info on how to use modules here . If you need software, try to find it at the vital-it website . You can load the software modules with module add [PATH] , to make it available: module add UHTS/Quality_control/fastqc/0.11.9 fastqc --help","title":"Using software"},{"location":"Assembly/week1/#raw-input-reads","text":"work to be done here We will probably make the reads available in a central place (and have multiple accessions). PacBio reads of Ler are at ERR3415825 and ERR3415826 module add UHTS/Analysis/sratoolkit/2.10.7 for SRA_ID in ERR3415825 ERR3415826 do prefetch $SRA_ID fastq-dump \\ --gzip \\ $SRA_ID done The whole genome Illumina reads of Ler: prefetch ERR3624574 fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files ERR3624574 Download also Illumina the paired-end RNA-seq reads of Ler: for SRA_ID in SRR5230991 SRR5230992 SRR5230993 do prefetch $SRA_ID fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files $SRA_ID done","title":"Raw input reads"},{"location":"Assembly/week1/#basic-read-statistics","text":"You have the following reads available: Whole genome PacBio reads Whole genome Illumina reads Whole transcriptome Illumina reads Exercise: Run fastqc on each of the fastq files. To do this, first look up the required module at https://www.vital-it.ch/services , and create a script to run fastqc using SLURM\u2019s sbatch . using SLURM A tutorial about job submission using SLURM on the IBU cluster you can find here . Questions: What are the read lengths of the different datasets? What kind of coverage do you expect from the Pacbio and the Illumina WGS reads? ( hint : lookup the expected genome size of Arabidopsis thaliana ) Do all datasets have information on base quality?","title":"Basic read statistics"},{"location":"Assembly/week1/#perform-k-mer-counting","text":"Exercise: Go to http://qb.cshl.edu/genomescope/ , and follow their suggestions on counting k-mers ( jellyfish count ) and creating a histogram ( jellyfish histo ). Other than what is suggested on the genomescope website, you can use the options -s 5G (hash of 5Gb) and -t 4 (4 threads). The other options you can keep the same as suggested on the website. Again, submit the commands from within a script using sbatch . jellyfish count only accepts uncompressed input Your fastq files are compressed (i.e. ending with fastq.gz ). jellyfish can not read those directly, but you can provide the files with process substitution of zcat , i.e.: jellyfish count \\ [ OPTIONS ] \\ < ( zcat myreads.fastq.gz ) \\ < ( zcat myotherreads.fastq.gz ) Questions: Upload your histo file to the website, checkout the plots and answer the following questions: Why are we using canonical k-mers? Is the estimated genome size expected? Is the percentage of heterozygousity expected?","title":"Perform k-mer counting"},{"location":"Assembly/week2/","text":"Introduction Today we will start with generating three assemblies: Whole genome assembly using flye Whole genome assembly using canu Whole transcriptome assembly using Trinity Note that assemblies are generally compute intense, particularly in terms of memory. So, your jobs will take at least a few hours to complete and require quite some memory. While flye and Trinity run within a single job, canu creates different job steps for you. This means that for flye and Trinity you will need to reserve the appropriate CPU and memory by using the sbatch options. The canu command itself runs very quickly, and it submits different jobs automatically that are needed to complete the canu assembly. flye assembly Generate and run a script for performing the flye assembly from the pacbio reads that you can submit using sbatch . Use (amongst others) the following sbatch options: #SBATCH --time=06:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=16 Hint Find the usage of flye here canu assembly Generate and run a script for performing the canu assembly from the pacbio reads. Have a look here to get started. In addition, you will need some specific options to tell canu how to submit to the cluster, these are: gridEngineResourceOption : the options how canu can specify resources within a job. For SLURM this is: gridEngineResourceOption=\"--cpus-per-task=THREADS --mem-per-cpu=MEMORY\" gridOptions : options you would otherwise add to the sbatch options, e.g.: gridOptions=\"--partion=pall --mail-user=mymail@students.unibe.ch\" More information on grid configuration here As canu creates the jobs for you, you can submit the canu command itself with limited resources, e.g.: #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1 Trinity assembly Find information on how to run Trinity here . You can provide multiple fastq files if they are separated by a comma. You can use (amongst others) the following sbatch options: #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12","title":"Week 2 - assembly"},{"location":"Assembly/week2/#introduction","text":"Today we will start with generating three assemblies: Whole genome assembly using flye Whole genome assembly using canu Whole transcriptome assembly using Trinity Note that assemblies are generally compute intense, particularly in terms of memory. So, your jobs will take at least a few hours to complete and require quite some memory. While flye and Trinity run within a single job, canu creates different job steps for you. This means that for flye and Trinity you will need to reserve the appropriate CPU and memory by using the sbatch options. The canu command itself runs very quickly, and it submits different jobs automatically that are needed to complete the canu assembly.","title":"Introduction"},{"location":"Assembly/week2/#flye-assembly","text":"Generate and run a script for performing the flye assembly from the pacbio reads that you can submit using sbatch . Use (amongst others) the following sbatch options: #SBATCH --time=06:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=16 Hint Find the usage of flye here","title":"flye assembly"},{"location":"Assembly/week2/#canu-assembly","text":"Generate and run a script for performing the canu assembly from the pacbio reads. Have a look here to get started. In addition, you will need some specific options to tell canu how to submit to the cluster, these are: gridEngineResourceOption : the options how canu can specify resources within a job. For SLURM this is: gridEngineResourceOption=\"--cpus-per-task=THREADS --mem-per-cpu=MEMORY\" gridOptions : options you would otherwise add to the sbatch options, e.g.: gridOptions=\"--partion=pall --mail-user=mymail@students.unibe.ch\" More information on grid configuration here As canu creates the jobs for you, you can submit the canu command itself with limited resources, e.g.: #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1","title":"canu assembly"},{"location":"Assembly/week2/#trinity-assembly","text":"Find information on how to run Trinity here . You can provide multiple fastq files if they are separated by a comma. You can use (amongst others) the following sbatch options: #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12","title":"Trinity assembly"},{"location":"Assembly/week3/","text":"Run containers Some software requires very specific dependencies and can not be installed as a module. These kind of software are available in containers. You can find them (like the modules) through the vital-it website . You can run the containers with singularity . The syntax is like this: singularity exec --bind [path to working directory] [path to container] [command] . You can find more information on the container of interest like this: # example for Merqury: singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif Running the program merqury.sh inside this container would look like: WORKDIR = /path/to/work/directory singularity exec \\ --bind $WORKDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ --help Note that singularity is not available on the head node (binfservms01). Therefore you can run singularity only within a job on any other node on the cluster. To quickly check the helper you can use: srun singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif To run the container (or any larger process) it\u2019s best to submit it as a job with sbatch . For this create a script (e.g. merqury.sh ): #!/usr/bin/env bash #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --job-name=merqury PROJDIR = /path/to/project/dir singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test And submit it with: sbatch merqury.sh quast #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_error_%j.txt #SBATCH --job-name=p651_quast #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/quast cd $PROJDIR /quality_control/quast singularity exec \\ --bind $PROJDIR \\ $PROJDIR /../containers/quast_5.1.0rc1.sif \\ quast.py \\ --output-dir $PROJDIR /quality_control/quast \\ --min-contig 100 \\ --threads ${ SLURM_CPUS_PER_TASK } \\ --eukaryote \\ --est-ref-size 135000000 \\ --labels flye,canu \\ $PROJDIR /assemblies/flye.fasta \\ $PROJDIR /assemblies/canu.fasta Run BUSCO #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_error_%j.txt #SBATCH --job-name=p651_busco #SBATCH --partition=pall # module add UHTS/Analysis/busco/4.1.4 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler # export AUGUSTUS_CONFIG_PATH=$PROJDIR/augustus_config # brassicales_odb10 singularity run \\ --bind $PROJDIR \\ $PROJDIR /../containers/busco_5.1.3--pyhdfd78af_0.sif \\ busco \\ -i $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta \\ -l brassicales_odb10 \\ -o BUSCO \\ -m genome \\ --cpu ${ SLURM_CPUS_PER_TASK } \\ --out_path $PROJDIR /Ler-deNovoAssembly_flye/ \\ --download_path $PROJDIR \\ --force merqury Look at: https://kat.readthedocs.io/en/latest/walkthrough.html#genome-assembly-analysis-using-k-mer-spectra #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_error_%j.txt #SBATCH --job-name=p651_merqury #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/meryl mkdir -p $PROJDIR /quality_control/merqury mkdir -p $PROJDIR /assemblies ln -s $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta $PROJDIR /assemblies/flye.fasta ln -s $PROJDIR /Ler-deNovoAssembly_canu/Ler.contigs.fasta $PROJDIR /assemblies/canu.fasta # singularity exec \\ # --bind $PROJDIR \\ # /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ # best_k.sh 135000000 singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ meryl \\ k = 18 \\ count output \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /reads/ERR3624574_1.fastq.gz $PROJDIR /reads/ERR3624574_2.fastq.gz cd $PROJDIR /quality_control/merqury/ singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test","title":"Week 3 - assembly evaluation"},{"location":"Assembly/week3/#run-containers","text":"Some software requires very specific dependencies and can not be installed as a module. These kind of software are available in containers. You can find them (like the modules) through the vital-it website . You can run the containers with singularity . The syntax is like this: singularity exec --bind [path to working directory] [path to container] [command] . You can find more information on the container of interest like this: # example for Merqury: singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif Running the program merqury.sh inside this container would look like: WORKDIR = /path/to/work/directory singularity exec \\ --bind $WORKDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ --help Note that singularity is not available on the head node (binfservms01). Therefore you can run singularity only within a job on any other node on the cluster. To quickly check the helper you can use: srun singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif To run the container (or any larger process) it\u2019s best to submit it as a job with sbatch . For this create a script (e.g. merqury.sh ): #!/usr/bin/env bash #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --job-name=merqury PROJDIR = /path/to/project/dir singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test And submit it with: sbatch merqury.sh","title":"Run containers"},{"location":"Assembly/week3/#quast","text":"#!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_error_%j.txt #SBATCH --job-name=p651_quast #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/quast cd $PROJDIR /quality_control/quast singularity exec \\ --bind $PROJDIR \\ $PROJDIR /../containers/quast_5.1.0rc1.sif \\ quast.py \\ --output-dir $PROJDIR /quality_control/quast \\ --min-contig 100 \\ --threads ${ SLURM_CPUS_PER_TASK } \\ --eukaryote \\ --est-ref-size 135000000 \\ --labels flye,canu \\ $PROJDIR /assemblies/flye.fasta \\ $PROJDIR /assemblies/canu.fasta","title":"quast"},{"location":"Assembly/week3/#run-busco","text":"#!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_error_%j.txt #SBATCH --job-name=p651_busco #SBATCH --partition=pall # module add UHTS/Analysis/busco/4.1.4 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler # export AUGUSTUS_CONFIG_PATH=$PROJDIR/augustus_config # brassicales_odb10 singularity run \\ --bind $PROJDIR \\ $PROJDIR /../containers/busco_5.1.3--pyhdfd78af_0.sif \\ busco \\ -i $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta \\ -l brassicales_odb10 \\ -o BUSCO \\ -m genome \\ --cpu ${ SLURM_CPUS_PER_TASK } \\ --out_path $PROJDIR /Ler-deNovoAssembly_flye/ \\ --download_path $PROJDIR \\ --force","title":"Run BUSCO"},{"location":"Assembly/week3/#merqury","text":"Look at: https://kat.readthedocs.io/en/latest/walkthrough.html#genome-assembly-analysis-using-k-mer-spectra #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_error_%j.txt #SBATCH --job-name=p651_merqury #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/meryl mkdir -p $PROJDIR /quality_control/merqury mkdir -p $PROJDIR /assemblies ln -s $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta $PROJDIR /assemblies/flye.fasta ln -s $PROJDIR /Ler-deNovoAssembly_canu/Ler.contigs.fasta $PROJDIR /assemblies/canu.fasta # singularity exec \\ # --bind $PROJDIR \\ # /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ # best_k.sh 135000000 singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ meryl \\ k = 18 \\ count output \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /reads/ERR3624574_1.fastq.gz $PROJDIR /reads/ERR3624574_2.fastq.gz cd $PROJDIR /quality_control/merqury/ singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test","title":"merqury"},{"location":"Assembly/week4/","text":"Run nucmer and mummer #!/usr/bin/env bash module add UHTS/Analysis/mummer/4.0.0beta1 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler REFERENCE = $PROJDIR /assemblies/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa FLYE = $PROJDIR /assemblies/flye.fasta CANU = $PROJDIR /assemblies/canu.fasta mkdir -p $PROJDIR /quality_control/mummer/ ## gnu plot installed with conda ## conda install -c conda-forge gnuplot for GENOME in $FLYE $CANU do BASE = ` basename $GENOME | cut -f 1 -d '.' ` nucmer \\ --prefix = $PROJDIR /quality_control/mummer/Ler_ ${ BASE } \\ --breaklen 1000 \\ --mincluster 1000 \\ $REFERENCE \\ $GENOME mummerplot \\ $PROJDIR /quality_control/mummer/Ler_ ${ BASE } .delta \\ -R $REFERENCE \\ -Q $GENOME \\ --filter \\ -p $PROJDIR /quality_control/mummer/Ler_ ${ BASE } _plot \\ -t png \\ --fat \\ --large \\ --layout done","title":"Week 4 - comparing genomes"},{"location":"Assembly/week4/#run-nucmer-and-mummer","text":"#!/usr/bin/env bash module add UHTS/Analysis/mummer/4.0.0beta1 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler REFERENCE = $PROJDIR /assemblies/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa FLYE = $PROJDIR /assemblies/flye.fasta CANU = $PROJDIR /assemblies/canu.fasta mkdir -p $PROJDIR /quality_control/mummer/ ## gnu plot installed with conda ## conda install -c conda-forge gnuplot for GENOME in $FLYE $CANU do BASE = ` basename $GENOME | cut -f 1 -d '.' ` nucmer \\ --prefix = $PROJDIR /quality_control/mummer/Ler_ ${ BASE } \\ --breaklen 1000 \\ --mincluster 1000 \\ $REFERENCE \\ $GENOME mummerplot \\ $PROJDIR /quality_control/mummer/Ler_ ${ BASE } .delta \\ -R $REFERENCE \\ -Q $GENOME \\ --filter \\ -p $PROJDIR /quality_control/mummer/Ler_ ${ BASE } _plot \\ -t png \\ --fat \\ --large \\ --layout done","title":"Run nucmer and mummer"}]}