{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Course website Material This website Zoom meeting (through mail) Google doc (through mail) Slack channel Learning outcomes General learning outcomes After this course, you will be able to: Learning outcomes explained To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn. Learning experiences To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only. Exercises Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different. Asking questions During lectures, you are encouraged to raise your hand if you have questions (if in-person), or use the Zoom functionality (if online). Find the buttons in the participants list (\u2018Participants\u2019 button): Alternatively, (depending on your zoom version or OS) use the \u2018Reactions\u2019 button: A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option: The teacher will review the answers, and add/modify if necessary. If you\u2019re really stuck and need specific tutor support, write the teachers or helpers personally. To summarise: During lectures: raise hand/zoom functionality Personal interest questions: #background During exercises: #q-and-a on slack","title":"Home"},{"location":"#course-website","text":"","title":"Course website"},{"location":"#material","text":"This website Zoom meeting (through mail) Google doc (through mail) Slack channel","title":"Material"},{"location":"#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"#general-learning-outcomes","text":"After this course, you will be able to:","title":"General learning outcomes"},{"location":"#learning-outcomes-explained","text":"To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.","title":"Learning outcomes explained"},{"location":"#learning-experiences","text":"To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.","title":"Learning experiences"},{"location":"#exercises","text":"Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.","title":"Exercises"},{"location":"#asking-questions","text":"During lectures, you are encouraged to raise your hand if you have questions (if in-person), or use the Zoom functionality (if online). Find the buttons in the participants list (\u2018Participants\u2019 button): Alternatively, (depending on your zoom version or OS) use the \u2018Reactions\u2019 button: A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option: The teacher will review the answers, and add/modify if necessary. If you\u2019re really stuck and need specific tutor support, write the teachers or helpers personally. To summarise: During lectures: raise hand/zoom functionality Personal interest questions: #background During exercises: #q-and-a on slack","title":"Asking questions"},{"location":"course_schedule/","text":"start end topic 10:00 10:30 coffee! 12:00 13:00 lunch! Generate markdown tables at tablesgenerator.com","title":"Course schedule"},{"location":"precourse/","text":"UNIX As is stated in the course prerequisites at the announcement web page , we expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial . Software We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory. Before the course, make sure you can comfortably work on a remote server. This means that you can approach it through the shell, modify scripts and transfer files. We can recommend atom for Linux and Mac, and Notepad ++ in combination with MobaXterm for Windows. We will be visualising our results with IGV. Therefore, install in your computer: mac OS/Linux SSH and scripting: Atom with packages like: terminus and ftp-remote-edit Transferring files: FileZilla Integrative Genomics Viewer (IGV) Windows SSH and scripting: MobaXterm and/or Notepad++ with the plugin NppFTP Transferring files: FileZilla Integrative Genomics Viewer (IGV)","title":"Precourse preparations"},{"location":"precourse/#unix","text":"As is stated in the course prerequisites at the announcement web page , we expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial .","title":"UNIX"},{"location":"precourse/#software","text":"We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory. Before the course, make sure you can comfortably work on a remote server. This means that you can approach it through the shell, modify scripts and transfer files. We can recommend atom for Linux and Mac, and Notepad ++ in combination with MobaXterm for Windows. We will be visualising our results with IGV. Therefore, install in your computer: mac OS/Linux SSH and scripting: Atom with packages like: terminus and ftp-remote-edit Transferring files: FileZilla Integrative Genomics Viewer (IGV) Windows SSH and scripting: MobaXterm and/or Notepad++ with the plugin NppFTP Transferring files: FileZilla Integrative Genomics Viewer (IGV)","title":"Software"},{"location":"Assembly/week1/","text":"Download the reads PacBio reads of Ler are at ERR3415825 and ERR3415826 module add UHTS/Analysis/sratoolkit/2.10.7 for SRA_ID in ERR3415825 ERR3415826 do prefetch $SRA_ID fastq-dump \\ --gzip \\ $SRA_ID done The whole genome Illumina reads of Ler: prefetch ERR3624574 fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files ERR3624574 Download also Illumina the paired-end RNA-seq reads of Ler: for SRA_ID in SRR5230991 SRR5230992 SRR5230993 do prefetch $SRA_ID fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files $SRA_ID done Using software On the IBU cluster software is available in modules or containers. More info on how to use modules here . If you need software, try to find it at the vital-it website . It will either be a module, which you can load with module add [PATH] , e.g.: module add UHTS/Quality_control/fastqc/0.11.9 fastqc --help or a container, which you can also find at the vital-it website (more information on that in week3 ). Basic read statistics You have the following reads available: Whole genome PacBio reads Whole genome Illumina reads Whole transcriptome Illumina reads Exercise: Run fastqc on each of the fastq files. To do this, first look up the required module at https://www.vital-it.ch/services , and create a script to run fastqc using SLURM\u2019s sbatch . using SLURM A tutorial about job submission using SLURM on the IBU cluster you can find here . Questions: What are the read lengths of the different datasets? What kind of coverage do you expect from the Pacbio and the Illumina WGS reads? ( hint : lookup the expected genome size of Arabidopsis thaliana ) Do all datasets have information on base quality? Perform k-mer counting Exercise: Go to http://qb.cshl.edu/genomescope/ , and follow their suggestions on counting k-mers ( jellyfish count ) and creating a histogram ( jellyfish histo ). Other than what is suggested on the genomescope website, you can use the options -s 5G (hash of 5Gb) and -t 4 (4 threads). The other options you can keep the same as suggested on the website. Again, submit the commands from within a script using sbatch . jellyfish count only accepts uncompressed input Your fastq files are compressed (i.e. ending with fastq.gz ). jellyfish can not read those directly, but you can provide the files with process substitution of zcat , i.e.: jellyfish count \\ [ OPTIONS ] \\ < ( zcat myreads.fastq.gz ) \\ < ( zcat myotherreads.fastq.gz ) Questions: Upload your histo file to the website, checkout the plots and answer the following questions: Why are we using canonical k-mers? Is the estimated genome size expected? Is the percentage of heterozygousity expected?","title":"Week 1 - reads & QC"},{"location":"Assembly/week1/#download-the-reads","text":"PacBio reads of Ler are at ERR3415825 and ERR3415826 module add UHTS/Analysis/sratoolkit/2.10.7 for SRA_ID in ERR3415825 ERR3415826 do prefetch $SRA_ID fastq-dump \\ --gzip \\ $SRA_ID done The whole genome Illumina reads of Ler: prefetch ERR3624574 fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files ERR3624574 Download also Illumina the paired-end RNA-seq reads of Ler: for SRA_ID in SRR5230991 SRR5230992 SRR5230993 do prefetch $SRA_ID fastq-dump \\ --gzip \\ --defline-seq '@$sn[_$rn]/$ri' \\ --split-files $SRA_ID done","title":"Download the reads"},{"location":"Assembly/week1/#using-software","text":"On the IBU cluster software is available in modules or containers. More info on how to use modules here . If you need software, try to find it at the vital-it website . It will either be a module, which you can load with module add [PATH] , e.g.: module add UHTS/Quality_control/fastqc/0.11.9 fastqc --help or a container, which you can also find at the vital-it website (more information on that in week3 ).","title":"Using software"},{"location":"Assembly/week1/#basic-read-statistics","text":"You have the following reads available: Whole genome PacBio reads Whole genome Illumina reads Whole transcriptome Illumina reads Exercise: Run fastqc on each of the fastq files. To do this, first look up the required module at https://www.vital-it.ch/services , and create a script to run fastqc using SLURM\u2019s sbatch . using SLURM A tutorial about job submission using SLURM on the IBU cluster you can find here . Questions: What are the read lengths of the different datasets? What kind of coverage do you expect from the Pacbio and the Illumina WGS reads? ( hint : lookup the expected genome size of Arabidopsis thaliana ) Do all datasets have information on base quality?","title":"Basic read statistics"},{"location":"Assembly/week1/#perform-k-mer-counting","text":"Exercise: Go to http://qb.cshl.edu/genomescope/ , and follow their suggestions on counting k-mers ( jellyfish count ) and creating a histogram ( jellyfish histo ). Other than what is suggested on the genomescope website, you can use the options -s 5G (hash of 5Gb) and -t 4 (4 threads). The other options you can keep the same as suggested on the website. Again, submit the commands from within a script using sbatch . jellyfish count only accepts uncompressed input Your fastq files are compressed (i.e. ending with fastq.gz ). jellyfish can not read those directly, but you can provide the files with process substitution of zcat , i.e.: jellyfish count \\ [ OPTIONS ] \\ < ( zcat myreads.fastq.gz ) \\ < ( zcat myotherreads.fastq.gz ) Questions: Upload your histo file to the website, checkout the plots and answer the following questions: Why are we using canonical k-mers? Is the estimated genome size expected? Is the percentage of heterozygousity expected?","title":"Perform k-mer counting"},{"location":"Assembly/week2/","text":"Introduction Today we will start with generating three assemblies: Whole genome assembly using flye Whole genome assembly using canu Whole transcriptome assembly using Trinity Note that assemblies are generally compute intense, particularly in terms of memory. So, your jobs will take at least a few hours to complete and require quite some memory. While flye and Trinity run within a single job, canu creates different job steps for you. This means that for flye and Trinity you will need to reserve the appropriate CPU and memory by using the sbatch options. The canu command itself runs very quickly, and it submits different jobs automatically that are needed to complete the canu assembly. flye assembly Generate and run a script for performing the flye assembly from the pacbio reads that you can submit using sbatch . Use (amongst others) the following sbatch options: #SBATCH --time=06:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=16 Hint Find the usage of flye here canu assembly Generate and run a script for performing the canu assembly from the pacbio reads. Have a look here to get started. In addition, you will need some specific options to tell canu how to submit to the cluster, these are: gridEngineResourceOption : the options how canu can specify resources within a job. For SLURM this is: gridEngineResourceOption=\"--cpus-per-task=THREADS --mem-per-cpu=MEMORY\" gridOptions : options you would otherwise add to the sbatch options, e.g.: gridOptions=\"--partion=pall --mail-user=mymail@students.unibe.ch\" More information on grid configuration here As canu creates the jobs for you, you can submit the canu command itself with limited resources, e.g.: #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1 Trinity assembly Find information on how to run Trinity here . You can provide multiple fastq files if they are separated by a comma. You can use (amongst others) the following sbatch options: #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12","title":"Week 2 - assembly"},{"location":"Assembly/week2/#introduction","text":"Today we will start with generating three assemblies: Whole genome assembly using flye Whole genome assembly using canu Whole transcriptome assembly using Trinity Note that assemblies are generally compute intense, particularly in terms of memory. So, your jobs will take at least a few hours to complete and require quite some memory. While flye and Trinity run within a single job, canu creates different job steps for you. This means that for flye and Trinity you will need to reserve the appropriate CPU and memory by using the sbatch options. The canu command itself runs very quickly, and it submits different jobs automatically that are needed to complete the canu assembly.","title":"Introduction"},{"location":"Assembly/week2/#flye-assembly","text":"Generate and run a script for performing the flye assembly from the pacbio reads that you can submit using sbatch . Use (amongst others) the following sbatch options: #SBATCH --time=06:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=16 Hint Find the usage of flye here","title":"flye assembly"},{"location":"Assembly/week2/#canu-assembly","text":"Generate and run a script for performing the canu assembly from the pacbio reads. Have a look here to get started. In addition, you will need some specific options to tell canu how to submit to the cluster, these are: gridEngineResourceOption : the options how canu can specify resources within a job. For SLURM this is: gridEngineResourceOption=\"--cpus-per-task=THREADS --mem-per-cpu=MEMORY\" gridOptions : options you would otherwise add to the sbatch options, e.g.: gridOptions=\"--partion=pall --mail-user=mymail@students.unibe.ch\" More information on grid configuration here As canu creates the jobs for you, you can submit the canu command itself with limited resources, e.g.: #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1","title":"canu assembly"},{"location":"Assembly/week2/#trinity-assembly","text":"Find information on how to run Trinity here . You can provide multiple fastq files if they are separated by a comma. You can use (amongst others) the following sbatch options: #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12","title":"Trinity assembly"},{"location":"Assembly/week3/","text":"Run containers Some software requires very specific dependencies and can not be installed as a module. These kind of software are available in containers. You can find them (like the modules) through the vital-it website . You can run the containers with singularity . The syntax is like this: singularity exec --bind [path to working directory] [path to container] [command] . You can find more information on the container of interest like this: # example for Merqury: singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif Running the program merqury.sh inside this container would look like: WORKDIR = /path/to/work/directory singularity exec \\ --bind $WORKDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ --help Note that singularity is not available on the head node (binfservms01). Therefore you can run singularity only within a job on any other node on the cluster. To quickly check the helper you can use: srun singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif To run the container (or any larger process) it\u2019s best to submit it as a job with sbatch . For this create a script (e.g. merqury.sh ): #!/usr/bin/env bash #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --job-name=merqury PROJDIR = /path/to/project/dir singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test And submit it with: sbatch merqury.sh quast #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_error_%j.txt #SBATCH --job-name=p651_quast #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/quast cd $PROJDIR /quality_control/quast singularity exec \\ --bind $PROJDIR \\ $PROJDIR /../containers/quast_5.1.0rc1.sif \\ quast.py \\ --output-dir $PROJDIR /quality_control/quast \\ --min-contig 100 \\ --threads ${ SLURM_CPUS_PER_TASK } \\ --eukaryote \\ --est-ref-size 135000000 \\ --labels flye,canu \\ $PROJDIR /assemblies/flye.fasta \\ $PROJDIR /assemblies/canu.fasta Run BUSCO #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_error_%j.txt #SBATCH --job-name=p651_busco #SBATCH --partition=pall # module add UHTS/Analysis/busco/4.1.4 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler # export AUGUSTUS_CONFIG_PATH=$PROJDIR/augustus_config # brassicales_odb10 singularity run \\ --bind $PROJDIR \\ $PROJDIR /../containers/busco_5.1.3--pyhdfd78af_0.sif \\ busco \\ -i $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta \\ -l brassicales_odb10 \\ -o BUSCO \\ -m genome \\ --cpu ${ SLURM_CPUS_PER_TASK } \\ --out_path $PROJDIR /Ler-deNovoAssembly_flye/ \\ --download_path $PROJDIR \\ --force merqury Look at: https://kat.readthedocs.io/en/latest/walkthrough.html#genome-assembly-analysis-using-k-mer-spectra #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_error_%j.txt #SBATCH --job-name=p651_merqury #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/meryl mkdir -p $PROJDIR /quality_control/merqury mkdir -p $PROJDIR /assemblies ln -s $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta $PROJDIR /assemblies/flye.fasta ln -s $PROJDIR /Ler-deNovoAssembly_canu/Ler.contigs.fasta $PROJDIR /assemblies/canu.fasta # singularity exec \\ # --bind $PROJDIR \\ # /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ # best_k.sh 135000000 singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ meryl \\ k = 18 \\ count output \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /reads/ERR3624574_1.fastq.gz $PROJDIR /reads/ERR3624574_2.fastq.gz cd $PROJDIR /quality_control/merqury/ singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test","title":"Week 3 - assembly evaluation"},{"location":"Assembly/week3/#run-containers","text":"Some software requires very specific dependencies and can not be installed as a module. These kind of software are available in containers. You can find them (like the modules) through the vital-it website . You can run the containers with singularity . The syntax is like this: singularity exec --bind [path to working directory] [path to container] [command] . You can find more information on the container of interest like this: # example for Merqury: singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif Running the program merqury.sh inside this container would look like: WORKDIR = /path/to/work/directory singularity exec \\ --bind $WORKDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ --help Note that singularity is not available on the head node (binfservms01). Therefore you can run singularity only within a job on any other node on the cluster. To quickly check the helper you can use: srun singularity run-help /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif To run the container (or any larger process) it\u2019s best to submit it as a job with sbatch . For this create a script (e.g. merqury.sh ): #!/usr/bin/env bash #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --job-name=merqury PROJDIR = /path/to/project/dir singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test And submit it with: sbatch merqury.sh","title":"Run containers"},{"location":"Assembly/week3/#quast","text":"#!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/quast_error_%j.txt #SBATCH --job-name=p651_quast #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/quast cd $PROJDIR /quality_control/quast singularity exec \\ --bind $PROJDIR \\ $PROJDIR /../containers/quast_5.1.0rc1.sif \\ quast.py \\ --output-dir $PROJDIR /quality_control/quast \\ --min-contig 100 \\ --threads ${ SLURM_CPUS_PER_TASK } \\ --eukaryote \\ --est-ref-size 135000000 \\ --labels flye,canu \\ $PROJDIR /assemblies/flye.fasta \\ $PROJDIR /assemblies/canu.fasta","title":"quast"},{"location":"Assembly/week3/#run-busco","text":"#!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=12 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/busco_flye_error_%j.txt #SBATCH --job-name=p651_busco #SBATCH --partition=pall # module add UHTS/Analysis/busco/4.1.4 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler # export AUGUSTUS_CONFIG_PATH=$PROJDIR/augustus_config # brassicales_odb10 singularity run \\ --bind $PROJDIR \\ $PROJDIR /../containers/busco_5.1.3--pyhdfd78af_0.sif \\ busco \\ -i $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta \\ -l brassicales_odb10 \\ -o BUSCO \\ -m genome \\ --cpu ${ SLURM_CPUS_PER_TASK } \\ --out_path $PROJDIR /Ler-deNovoAssembly_flye/ \\ --download_path $PROJDIR \\ --force","title":"Run BUSCO"},{"location":"Assembly/week3/#merqury","text":"Look at: https://kat.readthedocs.io/en/latest/walkthrough.html#genome-assembly-analysis-using-k-mer-spectra #!/usr/bin/env bash #SBATCH --mail-user=geert.vangeest@bioinformatics.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --time=1-00:00:00 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=4 #SBATCH --output=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_output_%j.txt #SBATCH --error=/data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler/log/merqury_error_%j.txt #SBATCH --job-name=p651_merqury #SBATCH --partition=pall PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler mkdir -p $PROJDIR /quality_control/meryl mkdir -p $PROJDIR /quality_control/merqury mkdir -p $PROJDIR /assemblies ln -s $PROJDIR /Ler-deNovoAssembly_flye/assembly.fasta $PROJDIR /assemblies/flye.fasta ln -s $PROJDIR /Ler-deNovoAssembly_canu/Ler.contigs.fasta $PROJDIR /assemblies/canu.fasta # singularity exec \\ # --bind $PROJDIR \\ # /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ # best_k.sh 135000000 singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ meryl \\ k = 18 \\ count output \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /reads/ERR3624574_1.fastq.gz $PROJDIR /reads/ERR3624574_2.fastq.gz cd $PROJDIR /quality_control/merqury/ singularity exec \\ --bind $PROJDIR \\ /software/singularity/containers/Merqury-1.3-1.ubuntu20.sif \\ merqury.sh \\ $PROJDIR /quality_control/meryl/Ler.meryl \\ $PROJDIR /assemblies/flye.fasta \\ flye_test","title":"merqury"},{"location":"Assembly/week4/","text":"Run nucmer and mummer #!/usr/bin/env bash module add UHTS/Analysis/mummer/4.0.0beta1 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler REFERENCE = $PROJDIR /assemblies/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa FLYE = $PROJDIR /assemblies/flye.fasta CANU = $PROJDIR /assemblies/canu.fasta mkdir -p $PROJDIR /quality_control/mummer/ ## gnu plot installed with conda ## conda install -c conda-forge gnuplot for GENOME in $FLYE $CANU do BASE = ` basename $GENOME | cut -f 1 -d '.' ` nucmer \\ --prefix = $PROJDIR /quality_control/mummer/Ler_ ${ BASE } \\ --breaklen 1000 \\ --mincluster 1000 \\ $REFERENCE \\ $GENOME mummerplot \\ $PROJDIR /quality_control/mummer/Ler_ ${ BASE } .delta \\ -R $REFERENCE \\ -Q $GENOME \\ --filter \\ -p $PROJDIR /quality_control/mummer/Ler_ ${ BASE } _plot \\ -t png \\ --fat \\ --large \\ --layout done","title":"Week 4 - comparing genomes"},{"location":"Assembly/week4/#run-nucmer-and-mummer","text":"#!/usr/bin/env bash module add UHTS/Analysis/mummer/4.0.0beta1 PROJDIR = /data/projects/p651_Assembly_and_annotation_course_2021/assembly_Ler REFERENCE = $PROJDIR /assemblies/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa FLYE = $PROJDIR /assemblies/flye.fasta CANU = $PROJDIR /assemblies/canu.fasta mkdir -p $PROJDIR /quality_control/mummer/ ## gnu plot installed with conda ## conda install -c conda-forge gnuplot for GENOME in $FLYE $CANU do BASE = ` basename $GENOME | cut -f 1 -d '.' ` nucmer \\ --prefix = $PROJDIR /quality_control/mummer/Ler_ ${ BASE } \\ --breaklen 1000 \\ --mincluster 1000 \\ $REFERENCE \\ $GENOME mummerplot \\ $PROJDIR /quality_control/mummer/Ler_ ${ BASE } .delta \\ -R $REFERENCE \\ -Q $GENOME \\ --filter \\ -p $PROJDIR /quality_control/mummer/Ler_ ${ BASE } _plot \\ -t png \\ --fat \\ --large \\ --layout done","title":"Run nucmer and mummer"}]}